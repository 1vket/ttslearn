

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>ttslearn.tacotron &mdash; ttslearn 0.2.2 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/plot_directive.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ttslearn.tacotron.frontend.openjtalk.pp_symbols" href="generated/ttslearn.tacotron.frontend.openjtalk.pp_symbols.html" />
    <link rel="prev" title="ttslearn.wavenet.receptive_field_size" href="generated/ttslearn.wavenet.receptive_field_size.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> ttslearn
          

          
          </a>

          
            
            
              <div class="version">
                0.2.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Demo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch00_Quick-start.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch11_Advanced-demos.html">Advanced TTS demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="demo_server.html">Demo server (with streamlit)</a></li>
</ul>
<p class="caption"><span class="caption-text">Chapters</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch04_Python-SP.html">第4章 Python による音声信号処理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch05_DNNTTS.html">第5章 深層学習に基づく統計的パラメトリック音声合成</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch06_Recipe-DNNTTS.html">第6章: 日本語 DNN 音声合成システムの実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch07_WaveNet.html">第7章 WaveNet: 深層学習に基づく音声波形の生成モデル</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch08_Recipe-WaveNet.html">第8章 日本語 WaveNet 音声合成システムの実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch09_Tacotron.html">第9章 Tacotron 2: 一貫学習を狙った音声合成</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/ch10_Recipe-Tacotron.html">第10章 日本語Tacotronに基づく音声合成システムの実装</a></li>
</ul>
<p class="caption"><span class="caption-text">Core modules</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dsp.html">ttslearn.dsp</a></li>
<li class="toctree-l1"><a class="reference internal" href="dnntts.html">ttslearn.dnntts</a></li>
<li class="toctree-l1"><a class="reference internal" href="wavenet.html">ttslearn.wavenet</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ttslearn.tacotron</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tts">TTS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#text-processing-frontend">Text processing frontend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-ttslearn.tacotron.frontend.openjtalk">Open JTalk (Japanese)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.pp_symbols.html">ttslearn.tacotron.frontend.openjtalk.pp_symbols</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.text_to_sequence.html">ttslearn.tacotron.frontend.openjtalk.text_to_sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.sequence_to_text.html">ttslearn.tacotron.frontend.openjtalk.sequence_to_text</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.num_vocab.html">ttslearn.tacotron.frontend.openjtalk.num_vocab</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-ttslearn.tacotron.frontend.text">Text (English)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.text_to_sequence.html">ttslearn.tacotron.frontend.text.text_to_sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.sequence_to_text.html">ttslearn.tacotron.frontend.text.sequence_to_text</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.num_vocab.html">ttslearn.tacotron.frontend.text.num_vocab</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.encoder">Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.attention">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.decoder">Decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.postnet">Post-Net</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.tacotron2">Tacotron 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-ttslearn.tacotron.gen">Generation utility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/ttslearn.tacotron.gen.synthesis_griffin_lim.html">ttslearn.tacotron.gen.synthesis_griffin_lim</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/ttslearn.tacotron.gen.synthesis.html">ttslearn.tacotron.gen.synthesis</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pretrained.html">ttslearn.pretrained</a></li>
<li class="toctree-l1"><a class="reference internal" href="util.html">ttslearn.util</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_util.html">ttslearn.train_util</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced topics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="extra_recipes.html">Extra recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib.html">ttslearn.contrib</a></li>
</ul>
<p class="caption"><span class="caption-text">Meta information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change log</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ttslearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>ttslearn.tacotron</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/tacotron.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="ttslearn-tacotron">
<h1>ttslearn.tacotron<a class="headerlink" href="#ttslearn-tacotron" title="Permalink to this headline">¶</a></h1>
<p>Tacotron 2に基づく音声合成のためのモジュールです。</p>
<div class="section" id="tts">
<h2>TTS<a class="headerlink" href="#tts" title="Permalink to this headline">¶</a></h2>
<p>The TTS functionality is accessible from <code class="docutils literal notranslate"><span class="pre">ttslearn.tacotron.*</span></code></p>
<span class="target" id="module-ttslearn.tacotron.tts"></span><dl class="py class">
<dt id="ttslearn.tacotron.tts.Tacotron2TTS">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.tts.</span></code><code class="sig-name descname"><span class="pre">Tacotron2TTS</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tts.html#Tacotron2TTS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tts.Tacotron2TTS" title="Permalink to this definition">¶</a></dt>
<dd><p>Tacotron 2 based text-to-speech</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_dir</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – model directory. A pre-trained model (ID: <code class="docutils literal notranslate"><span class="pre">tacotron2</span></code>)
is used if None.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – cpu or cuda.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ttslearn.tacotron</span> <span class="kn">import</span> <span class="n">Tacotron2TTS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">engine</span> <span class="o">=</span> <span class="n">Tacotron2TTS</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wav</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">tts</span><span class="p">(</span><span class="s2">&quot;一貫学習にチャレンジしましょう！&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="ttslearn.tacotron.tts.Tacotron2TTS.set_device">
<code class="sig-name descname"><span class="pre">set_device</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tts.html#Tacotron2TTS.set_device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tts.Tacotron2TTS.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Set device for the TTS models</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – cpu or cuda.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="ttslearn.tacotron.tts.Tacotron2TTS.tts">
<code class="sig-name descname"><span class="pre">tts</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">text</span></em>, <em class="sig-param"><span class="pre">griffin_lim=False</span></em>, <em class="sig-param"><span class="pre">tqdm=&lt;class</span> <span class="pre">'tqdm.std.tqdm'&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tts.html#Tacotron2TTS.tts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tts.Tacotron2TTS.tts" title="Permalink to this definition">¶</a></dt>
<dd><p>Run TTS</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – Input text</p></li>
<li><p><strong>griffin_lim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>optional</em>) – Use Griffin-Lim algorithm or not. Defaults to False.</p></li>
<li><p><strong>tqdm</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#object" title="(in Python v3.11)"><em>object</em></a><em>, </em><em>optional</em>) – tqdm object. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>audio array (np.int16) and sampling rate (int)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#tuple" title="(in Python v3.11)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="text-processing-frontend">
<h2>Text processing frontend<a class="headerlink" href="#text-processing-frontend" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-ttslearn.tacotron.frontend.openjtalk">
<span id="open-jtalk-japanese"></span><h3>Open JTalk (Japanese)<a class="headerlink" href="#module-ttslearn.tacotron.frontend.openjtalk" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.pp_symbols.html#ttslearn.tacotron.frontend.openjtalk.pp_symbols" title="ttslearn.tacotron.frontend.openjtalk.pp_symbols"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pp_symbols</span></code></a></p></td>
<td><p>Extract phoneme + prosoody symbol sequence from input full-context labels</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.text_to_sequence.html#ttslearn.tacotron.frontend.openjtalk.text_to_sequence" title="ttslearn.tacotron.frontend.openjtalk.text_to_sequence"><code class="xref py py-obj docutils literal notranslate"><span class="pre">text_to_sequence</span></code></a></p></td>
<td><p>Convert phoneme + prosody symbols to sequence of numbers</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.sequence_to_text.html#ttslearn.tacotron.frontend.openjtalk.sequence_to_text" title="ttslearn.tacotron.frontend.openjtalk.sequence_to_text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sequence_to_text</span></code></a></p></td>
<td><p>Convert sequence of numbers to phoneme + prosody symbols</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.openjtalk.num_vocab.html#ttslearn.tacotron.frontend.openjtalk.num_vocab" title="ttslearn.tacotron.frontend.openjtalk.num_vocab"><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_vocab</span></code></a></p></td>
<td><p>Get number of vocabraries</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-ttslearn.tacotron.frontend.text">
<span id="text-english"></span><h3>Text (English)<a class="headerlink" href="#module-ttslearn.tacotron.frontend.text" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.text_to_sequence.html#ttslearn.tacotron.frontend.text.text_to_sequence" title="ttslearn.tacotron.frontend.text.text_to_sequence"><code class="xref py py-obj docutils literal notranslate"><span class="pre">text_to_sequence</span></code></a></p></td>
<td><p>Convert text to sequence of numbers</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.sequence_to_text.html#ttslearn.tacotron.frontend.text.sequence_to_text" title="ttslearn.tacotron.frontend.text.sequence_to_text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sequence_to_text</span></code></a></p></td>
<td><p>Convert sequence of numbers to text</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.frontend.text.num_vocab.html#ttslearn.tacotron.frontend.text.num_vocab" title="ttslearn.tacotron.frontend.text.num_vocab"><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_vocab</span></code></a></p></td>
<td><p>Get number of vocabraries</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="module-ttslearn.tacotron.encoder">
<span id="encoder"></span><h2>Encoder<a class="headerlink" href="#module-ttslearn.tacotron.encoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="ttslearn.tacotron.encoder.Encoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.encoder.</span></code><code class="sig-name descname"><span class="pre">Encoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_vocab</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">51</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/encoder.html#Encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.encoder.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder of Tacotron 2</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_vocab</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of vocabularies</p></li>
<li><p><strong>embed_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of embeddings</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden units</p></li>
<li><p><strong>conv_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of convolutional layers</p></li>
<li><p><strong>conv_channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of convolutional channels</p></li>
<li><p><strong>conv_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – size of convolutional kernel</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.encoder.Encoder.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_lens</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/encoder.html#Encoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.encoder.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seqs</strong> (<em>torch.Tensor</em>) – input sequences</p></li>
<li><p><strong>in_lens</strong> (<em>torch.Tensor</em>) – input sequence lengths</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>encoded sequences</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ttslearn.tacotron.attention">
<span id="attention"></span><h2>Attention<a class="headerlink" href="#module-ttslearn.tacotron.attention" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="ttslearn.tacotron.attention.BahdanauAttention">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.attention.</span></code><code class="sig-name descname"><span class="pre">BahdanauAttention</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#BahdanauAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.BahdanauAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bahdanau-style attention</p>
<p>This is an attention mechanism originally used in Tacotron.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of encoder outputs</p></li>
<li><p><strong>decoder_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of decoder outputs</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden state</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.attention.BahdanauAttention.reset">
<code class="sig-name descname"><span class="pre">reset</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#BahdanauAttention.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.BahdanauAttention.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the internal buffer</p>
</dd></dl>

<dl class="py method">
<dt id="ttslearn.tacotron.attention.BahdanauAttention.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_outs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_lens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#BahdanauAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.BahdanauAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_outs</strong> (<em>torch.FloatTensor</em>) – encoder outputs</p></li>
<li><p><strong>src_lens</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#list" title="(in Python v3.11)"><em>list</em></a>) – length of each input batch</p></li>
<li><p><strong>decoder_state</strong> (<em>torch.FloatTensor</em>) – decoder hidden state</p></li>
<li><p><strong>mask</strong> (<em>torch.FloatTensor</em>) – mask for padding</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="ttslearn.tacotron.attention.LocationSensitiveAttention">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.attention.</span></code><code class="sig-name descname"><span class="pre">LocationSensitiveAttention</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#LocationSensitiveAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.LocationSensitiveAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Location-sensitive attention</p>
<p>This is an attention mechanism used in Tacotron 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of encoder outputs</p></li>
<li><p><strong>decoder_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of decoder outputs</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden state</p></li>
<li><p><strong>conv_channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of channels of convolutional layer</p></li>
<li><p><strong>conv_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – size of convolutional kernel</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.attention.LocationSensitiveAttention.reset">
<code class="sig-name descname"><span class="pre">reset</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#LocationSensitiveAttention.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.LocationSensitiveAttention.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the internal buffer</p>
</dd></dl>

<dl class="py method">
<dt id="ttslearn.tacotron.attention.LocationSensitiveAttention.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_outs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_lens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_prev</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/attention.html#LocationSensitiveAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.attention.LocationSensitiveAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_outs</strong> (<em>torch.FloatTensor</em>) – encoder outputs</p></li>
<li><p><strong>src_lens</strong> (<a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#list" title="(in Python v3.11)"><em>list</em></a>) – length of each input batch</p></li>
<li><p><strong>decoder_state</strong> (<em>torch.FloatTensor</em>) – decoder hidden state</p></li>
<li><p><strong>att_prev</strong> (<em>torch.FloatTensor</em>) – previous attention weight</p></li>
<li><p><strong>mask</strong> (<em>torch.FloatTensor</em>) – mask for padding</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ttslearn.tacotron.decoder">
<span id="decoder"></span><h2>Decoder<a class="headerlink" href="#module-ttslearn.tacotron.decoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="ttslearn.tacotron.decoder.Prenet">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.decoder.</span></code><code class="sig-name descname"><span class="pre">Prenet</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/decoder.html#Prenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.decoder.Prenet" title="Permalink to this definition">¶</a></dt>
<dd><p>Pre-Net of Tacotron/Tacotron 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of input</p></li>
<li><p><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of pre-net layers</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden layer</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.decoder.Prenet.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/decoder.html#Prenet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.decoder.Prenet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – input tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="ttslearn.tacotron.decoder.Decoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.decoder.</span></code><code class="sig-name descname"><span class="pre">Decoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prenet_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prenet_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prenet_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zoneout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_conv_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_conv_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/decoder.html#Decoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.decoder.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Decoder of Tacotron 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of encoder hidden layer</p></li>
<li><p><strong>out_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of output</p></li>
<li><p><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of LSTM layers</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden layer</p></li>
<li><p><strong>prenet_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of pre-net layers</p></li>
<li><p><strong>prenet_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of pre-net hidden layer</p></li>
<li><p><strong>prenet_dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate of pre-net</p></li>
<li><p><strong>zoneout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – zoneout rate</p></li>
<li><p><strong>reduction_factor</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – reduction factor</p></li>
<li><p><strong>attention_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of attention hidden layer</p></li>
<li><p><strong>attention_conv_channel</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of attention convolution channels</p></li>
<li><p><strong>attention_conv_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – kernel size of attention convolution</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.decoder.Decoder.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_outs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_lens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/decoder.html#Decoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.decoder.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_outs</strong> (<em>torch.Tensor</em>) – encoder outputs</p></li>
<li><p><strong>in_lens</strong> (<em>torch.Tensor</em>) – input lengths</p></li>
<li><p><strong>decoder_targets</strong> (<em>torch.Tensor</em>) – decoder targets for teacher-forcing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple of outputs, stop token prediction, and attention weights</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#tuple" title="(in Python v3.11)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ttslearn.tacotron.postnet">
<span id="post-net"></span><h2>Post-Net<a class="headerlink" href="#module-ttslearn.tacotron.postnet" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="ttslearn.tacotron.postnet.Postnet">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.postnet.</span></code><code class="sig-name descname"><span class="pre">Postnet</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/postnet.html#Postnet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.postnet.Postnet" title="Permalink to this definition">¶</a></dt>
<dd><p>Post-Net of Tacotron 2</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of input</p></li>
<li><p><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of layers</p></li>
<li><p><strong>channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of channels</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – kernel size</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.postnet.Postnet.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/postnet.html#Postnet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.postnet.Postnet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>xs</strong> (<em>torch.Tensor</em>) – input sequence</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output sequence</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ttslearn.tacotron.tacotron2">
<span id="tacotron-2"></span><h2>Tacotron 2<a class="headerlink" href="#module-ttslearn.tacotron.tacotron2" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="ttslearn.tacotron.tacotron2.Tacotron2">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">ttslearn.tacotron.tacotron2.</span></code><code class="sig-name descname"><span class="pre">Tacotron2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_vocab</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">51</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_conv_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_conv_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_conv_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_conv_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_conv_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_prenet_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_prenet_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_prenet_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_zoneout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postnet_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postnet_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postnet_kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postnet_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tacotron2.html#Tacotron2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tacotron2.Tacotron2" title="Permalink to this definition">¶</a></dt>
<dd><p>Tacotron 2</p>
<p>This implementation does not include the WaveNet vocoder of the Tacotron 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_vocab</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the size of vocabulary</p></li>
<li><p><strong>embed_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of embedding</p></li>
<li><p><strong>encoder_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden unit</p></li>
<li><p><strong>encoder_conv_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of convolution layers</p></li>
<li><p><strong>encoder_conv_channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of convolution channels</p></li>
<li><p><strong>encoder_conv_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – kernel size of convolution</p></li>
<li><p><strong>encoder_dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate of convolution</p></li>
<li><p><strong>attention_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden unit</p></li>
<li><p><strong>attention_conv_channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of convolution channels</p></li>
<li><p><strong>attention_conv_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – kernel size of convolution</p></li>
<li><p><strong>decoder_out_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of output</p></li>
<li><p><strong>decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of decoder layers</p></li>
<li><p><strong>decoder_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden unit</p></li>
<li><p><strong>decoder_prenet_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of prenet layers</p></li>
<li><p><strong>decoder_prenet_hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – dimension of hidden unit</p></li>
<li><p><strong>decoder_prenet_dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate of prenet</p></li>
<li><p><strong>decoder_zoneout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – zoneout rate</p></li>
<li><p><strong>postnet_layers</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of postnet layers</p></li>
<li><p><strong>postnet_channels</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – the number of postnet channels</p></li>
<li><p><strong>postnet_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – kernel size of postnet</p></li>
<li><p><strong>postnet_dropout</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – dropout rate of postnet</p></li>
<li><p><strong>reduction_factor</strong> (<a class="reference external" href="https://docs.python.org/dev/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – reduction factor</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="ttslearn.tacotron.tacotron2.Tacotron2.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_lens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tacotron2.html#Tacotron2.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tacotron2.Tacotron2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq</strong> (<em>torch.Tensor</em>) – input sequence</p></li>
<li><p><strong>in_lens</strong> (<em>torch.Tensor</em>) – input sequence lengths</p></li>
<li><p><strong>decoder_targets</strong> (<em>torch.Tensor</em>) – target sequence</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>tuple of outputs, outputs (after post-net), stop token prediction</dt><dd><p>and attention weights.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#tuple" title="(in Python v3.11)">tuple</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="ttslearn.tacotron.tacotron2.Tacotron2.inference">
<code class="sig-name descname"><span class="pre">inference</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ttslearn/tacotron/tacotron2.html#Tacotron2.inference"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ttslearn.tacotron.tacotron2.Tacotron2.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seq</strong> (<em>torch.Tensor</em>) – input sequence</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>tuple of outputs, outputs (after post-net), stop token prediction</dt><dd><p>and attention weights.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/dev/library/stdtypes.html#tuple" title="(in Python v3.11)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ttslearn.tacotron.gen">
<span id="generation-utility"></span><h2>Generation utility<a class="headerlink" href="#module-ttslearn.tacotron.gen" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.gen.synthesis_griffin_lim.html#ttslearn.tacotron.gen.synthesis_griffin_lim" title="ttslearn.tacotron.gen.synthesis_griffin_lim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">synthesis_griffin_lim</span></code></a></p></td>
<td><p>Synthesize waveform with Griffin-Lim algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/ttslearn.tacotron.gen.synthesis.html#ttslearn.tacotron.gen.synthesis" title="ttslearn.tacotron.gen.synthesis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">synthesis</span></code></a></p></td>
<td><p>Synthesize waveform</p></td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="generated/ttslearn.tacotron.frontend.openjtalk.pp_symbols.html" class="btn btn-neutral float-right" title="ttslearn.tacotron.frontend.openjtalk.pp_symbols" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="generated/ttslearn.wavenet.receptive_field_size.html" class="btn btn-neutral float-left" title="ttslearn.wavenet.receptive_field_size" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ryuichi Yamamoto.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>